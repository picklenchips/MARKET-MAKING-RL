{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.masked as masked\n",
    "import torch.distributions as ptd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20) (10, 20, 7)\n",
      "forward!\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n",
      "rets\n",
      "obs\n",
      "False False\n",
      "forward!\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rl_market/lib/python3.12/site-packages/torch/masked/maskedtensor/core.py:156: UserWarning: The PyTorch API of MaskedTensors is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.masked module for further information about the project.\n",
      "  warnings.warn((\"The PyTorch API of MaskedTensors is in prototype stage \"\n",
      "/opt/miniconda3/envs/rl_market/lib/python3.12/site-packages/torch/masked/maskedtensor/core.py:161: UserWarning: It is not recommended to create a MaskedTensor with a tensor that requires_grad. To avoid this, you can use data.clone().detach()\n",
      "  warnings.warn(\"It is not recommended to create a MaskedTensor with a tensor that requires_grad. \"\n"
     ]
    }
   ],
   "source": [
    "val_dim = 7\n",
    "act_dim = 4\n",
    "obs_dim = 5\n",
    "n_layers = 2\n",
    "layer_size = 64\n",
    "\n",
    "\"\"\" sooo masked arrays dont have matrix multiplication \n",
    "autograd support. so we need to completely pivot to the sparse library \n",
    "and instead use COO tensors, stored in either CSR or CSC format but IDK \n",
    "the difference... \"\"\"\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def np2torch(x, requires_grad=False, cast_double_to_float=True):\n",
    "    if isinstance(x, np.ma.MaskedArray):\n",
    "        mask = torch.from_numpy(~x.mask).to(device)\n",
    "        x = torch.from_numpy(x.data).to(device)\n",
    "        if requires_grad: x = x.float()\n",
    "        x = masked.as_masked_tensor(x, mask).to(device)\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "    else:\n",
    "        x = torch.Tensor(x).to(device)\n",
    "    if requires_grad:\n",
    "        x.requires_grad = True\n",
    "    elif cast_double_to_float and x.dtype == torch.float64:\n",
    "        x = x.float()\n",
    "    return x\n",
    "\n",
    "def torch2np(x: torch.Tensor | masked.MaskedTensor):\n",
    "    if isinstance(x, masked.MaskedTensor):\n",
    "        mask = ~x._masked_mask.detach().cpu().numpy()\n",
    "        data = x._masked_data.detach().cpu().numpy()\n",
    "        return np.ma.MaskedArray(data, mask=mask)\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "class MaskedSequential(nn.Sequential):\n",
    "    \"\"\" Version of nn.Sequential that can handle masked tensors \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        isMasked = isinstance(x, masked.MaskedTensor)\n",
    "        if isMasked:\n",
    "            mask = x._masked_mask[...,:1].to(device)\n",
    "            req_grad = x.requires_grad\n",
    "            x    = x._masked_data.to(device).requires_grad_(req_grad)\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "        if isMasked:\n",
    "            # account for different output, input dims\n",
    "            newshape = [1]*(len(x.shape)-1)+[x.shape[-1]]\n",
    "            mask = mask.repeat(*newshape).detach()\n",
    "            return x, mask\n",
    "        return x\n",
    "\n",
    "def build_mlp(input_size, output_size, n_layers, hidden_size, activation=nn.ReLU()):\n",
    "    \"\"\" Build multi-layer perception with n_layers hidden layers of size hidden_size \"\"\"\n",
    "    layers = [nn.Linear(input_size,hidden_size),activation]\n",
    "    for i in range(n_layers-1):\n",
    "        layers.append(nn.Linear(hidden_size,hidden_size))\n",
    "        layers.append(activation)\n",
    "    layers.append(nn.Linear(hidden_size, output_size))\n",
    "    return MaskedSequential(*layers)\n",
    "\n",
    "class BaselineNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for implementing Baseline network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = build_mlp(val_dim, 1, n_layers, layer_size)\n",
    "        self.optimizer = torch.optim.Adam(params=self.network.parameters(),lr=1e-3)\n",
    "    \n",
    "    def forward(self, observations: torch.Tensor | torch.masked.MaskedTensor):\n",
    "        if isinstance(observations, masked.MaskedTensor):\n",
    "            \"\"\" implement a masked tensor squeeze \"\"\"\n",
    "            print('forward!')\n",
    "            values, mask = self.network(observations)\n",
    "            req_grad = values.requires_grad\n",
    "            values = values.squeeze().clone().detach()\n",
    "            mask   = mask.squeeze()\n",
    "            return masked.masked_tensor(values, mask, requires_grad=req_grad).to(device)\n",
    "        return self.network(observations).squeeze()\n",
    "\n",
    "    def calculate_advantage(self, returns: np.ndarray | np.ma.MaskedArray, observations: np.ndarray | np.ma.MaskedArray) -> np.ndarray | np.ma.MaskedArray:\n",
    "        \"\"\" Compute advantages given returns \"\"\"\n",
    "        return returns - torch2np(self.forward(np2torch(observations)))\n",
    "\n",
    "    def update_baseline(self, returns: np.ndarray, observations: np.ndarray):\n",
    "        \"\"\" Gradient baseline to match returns \"\"\"\n",
    "        returns = np2torch(returns, True)\n",
    "        print('rets')\n",
    "        observations = np2torch(observations, True)\n",
    "        print('obs')\n",
    "        print(returns._masked_data.requires_grad, observations._masked_data.requires_grad)\n",
    "        baseline = self.forward(observations)\n",
    "        print(baseline._masked_data.requires_grad)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.mean((baseline - returns)**2)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "nb = 10\n",
    "nt = 20\n",
    "baseline = BaselineNetwork()\n",
    "v = np.ma.empty((nb, nt, val_dim))\n",
    "v[:] = np.ma.masked\n",
    "r = np.ma.empty((nb, nt))\n",
    "r[:] = np.ma.masked\n",
    "for i in range(nb):\n",
    "    length = int(np.random.rand(1)[0]*nt)\n",
    "    v[i, :length, :] = np.random.rand(length, val_dim)\n",
    "    r[i, :length] = np.random.rand(length)\n",
    "print(r.shape, v.shape)\n",
    "#advantages = baseline.calculate_advantage(r, v)\n",
    "#print(advantages.shape)\n",
    "#print(advantages)\n",
    "#for i in range(10):\n",
    "#    baseline.update_baseline(r, v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 269\u001b[0m\n\u001b[1;32m    267\u001b[0m P \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mpolicy\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(o, np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mMaskedArray), \u001b[38;5;28misinstance\u001b[39m(a, np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mMaskedArray))\n\u001b[0;32m--> 269\u001b[0m sampled, logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_log_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sampled, np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mMaskedArray), \u001b[38;5;28misinstance\u001b[39m(logprobs, np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mMaskedArray))\n\u001b[1;32m    271\u001b[0m returns \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mget_td_returns(r, v)\n",
      "Cell \u001b[0;32mIn[43], line 148\u001b[0m, in \u001b[0;36mGaussianPolicy.act\u001b[0;34m(self, observations, return_log_prob)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Return np.ndarray actions (and log probs)? from action distribution \"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m observations \u001b[38;5;241m=\u001b[39m np2torch(observations)\n\u001b[0;32m--> 148\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    150\u001b[0m sampled_actions \u001b[38;5;241m=\u001b[39m torch2np(actions)\n",
      "Cell \u001b[0;32mIn[43], line 161\u001b[0m, in \u001b[0;36mGaussianPolicy.action_distribution\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(means, masked\u001b[38;5;241m.\u001b[39mMaskedTensor):\n\u001b[1;32m    160\u001b[0m     means \u001b[38;5;241m=\u001b[39m means\u001b[38;5;241m.\u001b[39m_masked_data\u001b[38;5;241m.\u001b[39mnan_to_num(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_tril\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_market/lib/python3.12/site-packages/torch/distributions/multivariate_normal.py:137\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    131\u001b[0m     loc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m ):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc must be at least one-dimensional.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (covariance_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m (scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    140\u001b[0m         precision_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     ) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "val_dim = 7\n",
    "act_dim = 4\n",
    "obs_dim = 5\n",
    "n_layers = 2\n",
    "layer_size = 64\n",
    "\n",
    "\"\"\" masked arrays are of the dimension \n",
    "nbatch x ntime x nfeatures, where ntime varies across batches. \n",
    "can we implement a spare tensor version of this that might be more efficient? yes. will I? no. \"\"\"\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def np2torch(x, requires_grad=False, cast_double_to_float=True):\n",
    "    if isinstance(x, np.ma.MaskedArray):\n",
    "        mask = torch.from_numpy(~x.mask).to(device)\n",
    "        x = torch.from_numpy(x.data).to(device)\n",
    "        if requires_grad: x = x.float()\n",
    "        x = masked.as_masked_tensor(x, mask).to(device)\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "    else:\n",
    "        x = torch.Tensor(x).to(device)\n",
    "    if requires_grad:\n",
    "        x.requires_grad = True\n",
    "    elif cast_double_to_float and x.dtype == torch.float64:\n",
    "        x = x.float()\n",
    "    return x\n",
    "\n",
    "def torch2np(x: torch.Tensor | masked.MaskedTensor):\n",
    "    if isinstance(x, masked.MaskedTensor):\n",
    "        mask = ~x._masked_mask.detach().cpu().numpy()\n",
    "        data = x._masked_data.detach().cpu().numpy()\n",
    "        return np.ma.MaskedArray(data, mask=mask)\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "class MaskedSequential(nn.Sequential):\n",
    "    \"\"\" Version of nn.Sequential that can handle masked tensors \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        isMasked = isinstance(x, masked.MaskedTensor)\n",
    "        if isMasked:\n",
    "            mask = x._masked_mask[...,:1].to(device)\n",
    "            req_grad = x.requires_grad\n",
    "            x    = x._masked_data.to(device).requires_grad_(req_grad)\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "        if isMasked:\n",
    "            # account for different output, input dims\n",
    "            newshape = [1]*(len(x.shape)-1)+[x.shape[-1]]\n",
    "            mask = mask.repeat(*newshape).detach()\n",
    "            return x, mask\n",
    "        return x\n",
    "\n",
    "def build_mlp(input_size, output_size, n_layers, hidden_size, activation=nn.ReLU()):\n",
    "    \"\"\" Build multi-layer perception with n_layers hidden layers of size hidden_size \"\"\"\n",
    "    layers = [nn.Linear(input_size,hidden_size),activation]\n",
    "    for i in range(n_layers-1):\n",
    "        layers.append(nn.Linear(hidden_size,hidden_size))\n",
    "        layers.append(activation)\n",
    "    layers.append(nn.Linear(hidden_size, output_size))\n",
    "    return MaskedSequential(*layers)\n",
    "\n",
    "class BaselineNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for implementing Baseline network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = build_mlp(val_dim, 1, n_layers, layer_size)\n",
    "        self.optimizer = torch.optim.Adam(params=self.network.parameters(),lr=1e-3)\n",
    "    \n",
    "    def forward(self, observations: torch.Tensor | torch.masked.MaskedTensor):\n",
    "        if isinstance(observations, masked.MaskedTensor):\n",
    "            \"\"\" implement a masked tensor squeeze \"\"\"\n",
    "            print('forward!')\n",
    "            values, mask = self.network(observations)\n",
    "            req_grad = values.requires_grad\n",
    "            values = values.squeeze().clone().detach()\n",
    "            mask   = mask.squeeze()\n",
    "            return masked.masked_tensor(values, mask, requires_grad=req_grad).to(device)\n",
    "        return self.network(observations).squeeze()\n",
    "\n",
    "    def calculate_advantage(self, returns: np.ndarray | np.ma.MaskedArray, observations: np.ndarray | np.ma.MaskedArray) -> np.ndarray | np.ma.MaskedArray:\n",
    "        \"\"\" Compute advantages given returns \"\"\"\n",
    "        return returns - torch2np(self.forward(np2torch(observations)))\n",
    "\n",
    "    def update_baseline(self, returns: np.ndarray, observations: np.ndarray):\n",
    "        \"\"\" Gradient baseline to match returns \"\"\"\n",
    "        returns = np2torch(returns, True)\n",
    "        print('rets')\n",
    "        observations = np2torch(observations, True)\n",
    "        print('obs')\n",
    "        print(returns._masked_data.requires_grad, observations._masked_data.requires_grad)\n",
    "        baseline = self.forward(observations)\n",
    "        print(baseline._masked_data.requires_grad)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.mean((baseline - returns)**2)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def get_lengths(arr: np.ma.MaskedArray) -> np.ndarray:\n",
    "    \"\"\" print length of the true elements in arr \"\"\"\n",
    "    lengths = np.empty(arr.shape[0], dtype=int)\n",
    "    for i in range(arr.shape[0]):  # get last non-masked value in each row\n",
    "        trues = np.where(~arr.mask[i])[0]\n",
    "        if len(trues.shape) > 1:\n",
    "            trues = trues[:,0]  # we dont care about obs_dim\n",
    "        if not len(trues): \n",
    "            lengths[i] = 0\n",
    "            continue\n",
    "        lengths[i] = trues[-1]\n",
    "    return lengths\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, network, action_dim):\n",
    "        nn.Module.__init__(self)\n",
    "        self.network = network\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim), requires_grad=False)\n",
    "\n",
    "    def std(self):\n",
    "        \"\"\" Returns: torch.Tensor of shape [dim(action space)] \"\"\"\n",
    "        return torch.exp(self.log_std)\n",
    "\n",
    "    def log_probs(self, distribution: ptd.Distribution, actions: torch.Tensor | masked.MaskedTensor):\n",
    "        \"\"\" Return log probs of actions \"\"\"\n",
    "        if isinstance(actions, masked.MaskedTensor):\n",
    "            data = torch.nan_to_num(actions._masked_data, 1)\n",
    "            mask = actions._masked_mask[...,0]  # bc probs are 1D\n",
    "            req_grad = actions.requires_grad\n",
    "            probs = distribution.log_prob(data)\n",
    "            return masked.masked_tensor(probs.detach(), mask.detach(), requires_grad=req_grad)\n",
    "        return distribution.log_prob(actions)\n",
    "\n",
    "    def entropy(self, distribution: ptd.Distribution, observations: torch.Tensor | masked.MaskedTensor):\n",
    "        \"\"\" Return entropy of the distribution \"\"\"\n",
    "        entropy = distribution.entropy()\n",
    "        if not isinstance(observations, masked.MaskedTensor):\n",
    "            return entropy     # bc entropy is 1D\n",
    "        mask = observations._masked_mask[...,0]\n",
    "        req_grad = observations.requires_grad\n",
    "        return masked.masked_tensor(entropy.detach(), mask.detach(), requires_grad=req_grad)\n",
    "\n",
    "    def act(self, observations: np.ndarray | np.ma.MaskedArray, return_log_prob = False):\n",
    "        \"\"\" Return np.ndarray actions (and log probs)? from action distribution \"\"\"\n",
    "        observations = np2torch(observations)\n",
    "        distribution = self.action_distribution(observations)\n",
    "        actions = distribution.sample()\n",
    "        sampled_actions = torch2np(actions)\n",
    "        if return_log_prob:\n",
    "            log_probs = torch2np(self.log_probs(distribution, actions))\n",
    "            return sampled_actions, log_probs\n",
    "        return sampled_actions\n",
    "\n",
    "    def action_distribution(self, observations):\n",
    "        \"\"\" continuous action distribution for given observations \"\"\"\n",
    "        means = self.network(observations)\n",
    "        if isinstance(means, masked.MaskedTensor):\n",
    "            means = means._masked_data.nan_to_num(0)\n",
    "        return ptd.MultivariateNormal(means, scale_tril=torch.diag(self.std()))\n",
    "\n",
    "class PolicyGradient():\n",
    "    \"\"\"\n",
    "    Class for implementing a policy gradient algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Policy Gradient Class\n",
    "            - config: class with all parameters\n",
    "        \"\"\"\n",
    "        self.discount = 0.99\n",
    "        self.lambd = 0.95\n",
    "        # set the baseline (value) network (val_dim -> 1)\n",
    "        self.baseline = BaselineNetwork()\n",
    "        self.eps_clip = 0.9\n",
    "        self.do_clip = True\n",
    "        self.entropy_coef = 0.02\n",
    "        self.do_ppo = True\n",
    "        # option to intialize policy on startup? why not\n",
    "        network = build_mlp(obs_dim, act_dim, n_layers, layer_size)\n",
    "        self.policy = GaussianPolicy(network, act_dim)\n",
    "        self.optimizer = torch.optim.Adam(params=self.policy.parameters(),lr=1e-3)\n",
    "\n",
    "    def get_finals(self, returns: np.ndarray | np.ma.MaskedArray) -> np.ndarray:\n",
    "        \"\"\" Get final returns from batched returns of shape (nb x nt) \"\"\"\n",
    "        if not isinstance(returns, np.ma.MaskedArray):\n",
    "            return returns[:,-1]\n",
    "        finals = np.empty(returns.shape[0])\n",
    "        for i in range(returns.shape[0]):  # get last non-masked value in each row\n",
    "            trues = np.where(~returns.mask[i])[0]\n",
    "            if not len(trues):\n",
    "                finals[i] = 0\n",
    "                continue\n",
    "            finals[i] = returns[i, trues[-1]]\n",
    "        return finals\n",
    "\n",
    "    def get_returns(self, rewards: np.ndarray | np.ma.MaskedArray) -> np.ndarray | np.ma.MaskedArray:\n",
    "        \"\"\" Classic returns from batched rewards of shape (nb x nt) \"\"\"\n",
    "        isMasked = isinstance(rewards, np.ma.MaskedArray)\n",
    "        returns = np.empty_like(rewards)\n",
    "        if isMasked:\n",
    "            mask = rewards.mask\n",
    "            rewards = rewards.filled(0)   # we want to add when OR on the masks\n",
    "        returns[:, -1] = rewards[:, -1]\n",
    "        for t in reversed(range(rewards.shape[1]-1)):\n",
    "            returns[:, t] = rewards[:, t] + self.discount*rewards[:, t+1]\n",
    "        if isMasked:\n",
    "            returns = np.ma.masked_array(returns, mask)\n",
    "        return returns\n",
    "    \n",
    "    def get_td_returns(self, rewards: np.ndarray | np.ma.MaskedArray, observations: np.ndarray | np.ma.MaskedArray) -> np.ndarray | np.ma.MaskedArray:\n",
    "        \"\"\" Compute TD(Î») returns \"\"\"\n",
    "        isMasked = isinstance(rewards, np.ma.MaskedArray)\n",
    "        returns = np.empty_like(rewards)\n",
    "        values = torch2np(self.baseline.forward(np2torch(observations)))\n",
    "        if isMasked:\n",
    "            mask = rewards.mask\n",
    "            rewards = rewards.filled(0)  # we want to add when OR on the masks\n",
    "            values = values.filled(0)\n",
    "        returns[:, -1] = rewards[:, -1] + self.discount * values[:, -1]\n",
    "        for t in reversed(range(rewards.shape[1]-1)):\n",
    "            returns[:, t] = rewards[:, t] + self.discount * ((1 - self.lambd) * values[:, t+1] + self.lambd * returns[:, t+1])\n",
    "        if isMasked:\n",
    "            returns = np.ma.masked_array(returns, mask)\n",
    "        return returns\n",
    "\n",
    "    def get_advantages(self, returns, trajectories):\n",
    "        advantages = self.baseline.calculate_advantage(returns, trajectories)\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages, logprobs):\n",
    "        observations = np2torch(observations, True)\n",
    "        actions      = np2torch(actions, True)\n",
    "        advantages   = np2torch(advantages, True)\n",
    "        old_logprobs = np2torch(logprobs, True)\n",
    "        for arr in [observations, actions, advantages, old_logprobs]:\n",
    "            print(arr._masked_mask.requires_grad, arr._masked_data.requires_grad)\n",
    "        \n",
    "        distribution = self.policy.action_distribution(observations)\n",
    "        log_probs    = self.policy.log_probs(distribution, actions)\n",
    "        z_ratio      = torch.exp(log_probs - old_logprobs)\n",
    "        entropy_loss = self.policy.entropy(distribution, observations)\n",
    "        if self.do_clip:\n",
    "            clip_z   = torch.clip(z_ratio,1-self.eps_clip,1+self.eps_clip)\n",
    "            minimum  = torch.min(z_ratio*advantages,clip_z*advantages)\n",
    "        else:\n",
    "            minimum  = z_ratio * advantages\n",
    "        self.optimizer.zero_grad()\n",
    "        print(minimum._masked_mask.requires_grad, entropy_loss._masked_mask.requires_grad)\n",
    "        loss         = -torch.mean(minimum) - torch.mean(self.entropy_coef * entropy_loss)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "v = np.ma.empty((nb, nt, val_dim))\n",
    "v[:] = np.ma.masked\n",
    "r = np.ma.empty((nb, nt))\n",
    "r[:] = np.ma.masked\n",
    "for i in range(nb):\n",
    "    length = int(np.random.rand(1)[0]*nt)\n",
    "    v[i, :length, :] = np.random.rand(length, val_dim)\n",
    "    r[i, :length] = np.random.rand(length)\n",
    "\n",
    "a = v[:,:,:act_dim]\n",
    "o = v[:,:,:obs_dim]\n",
    "policy = PolicyGradient()\n",
    "P = policy.policy\n",
    "print(isinstance(o, np.ma.MaskedArray), isinstance(a, np.ma.MaskedArray))\n",
    "sampled, logprobs = P.act(o, return_log_prob=True)\n",
    "print(isinstance(sampled, np.ma.MaskedArray), isinstance(logprobs, np.ma.MaskedArray))\n",
    "returns = policy.get_td_returns(r, v)\n",
    "finals = policy.get_finals(returns)\n",
    "advantages = policy.get_advantages(returns, v)\n",
    "policy.update_policy(o, a, advantages, logprobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_market",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
